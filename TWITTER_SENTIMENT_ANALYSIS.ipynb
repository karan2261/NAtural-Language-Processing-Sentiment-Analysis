{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karan2261/Twitter-Sentiment-Analysis-using-NLP/blob/main/TWITTER_SENTIMENT_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zT-dDhX3GaO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2vawEfVhf7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b990a263-3ca5-4887-e3a0-47d0a36f09bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GDrQeghAceA"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_vtaRd4hE0"
      },
      "outputs": [],
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    cleaned_tokens = [token.text for token in doc if not token.is_punct and not token.is_stop and not token.like_num]\n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xH7l1_3BMMd"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv('twitter_training.csv')\n",
        "validation_data = pd.read_csv('twitter_validation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWYFYMFMBjO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ea766d-a42f-431b-e6b3-98c578b31319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data columns: Index(['2401', 'Borderlands', 'Positive',\n",
            "       'im getting on borderlands and i will murder you all ,'],\n",
            "      dtype='object')\n",
            "Validation data columns: Index(['3364', 'Facebook', 'Irrelevant',\n",
            "       'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data columns:\", train_data.columns)\n",
        "print(\"Validation data columns:\", validation_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWnHxEVKBPFH"
      },
      "outputs": [],
      "source": [
        "# Rename columns\n",
        "train_data.columns = ['id', 'source', 'label', 'text']\n",
        "validation_data.columns = ['id', 'source', 'label', 'text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nKCF26GBWxc"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing values in the text column\n",
        "train_data_cleaned = train_data.dropna(subset=['text'])\n",
        "validation_data_cleaned = validation_data.dropna(subset=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKqG68LfBwNL",
        "outputId": "60433d11-352d-4fbb-94ca-d391ffc41599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-eb95bc45f7b8>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data_cleaned['cleaned_text'] = train_data_cleaned['text'].apply(clean_text)\n"
          ]
        }
      ],
      "source": [
        "# Apply text cleaning\n",
        "train_data_cleaned['cleaned_text'] = train_data_cleaned['text'].apply(clean_text)\n",
        "validation_data_cleaned['cleaned_text'] = validation_data_cleaned['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "157o1JIbB0Br",
        "outputId": "1d1e5275-a0ce-4401-8db4-5b8101b6197f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Train Data Head:\n",
            "                                                text  \\\n",
            "0  I am coming to the borders and I will kill you...   \n",
            "1  im getting on borderlands and i will kill you ...   \n",
            "2  im coming on borderlands and i will murder you...   \n",
            "3  im getting on borderlands 2 and i will murder ...   \n",
            "4  im getting into borderlands and i can murder y...   \n",
            "\n",
            "                   cleaned_text  \n",
            "0           coming borders kill  \n",
            "1    m getting borderlands kill  \n",
            "2   m coming borderlands murder  \n",
            "3  m getting borderlands murder  \n",
            "4  m getting borderlands murder  \n",
            "Cleaned Train Data Tail:\n",
            "                                                    text  \\\n",
            "74676  Just realized that the Windows partition of my...   \n",
            "74677  Just realized that my Mac window partition is ...   \n",
            "74678  Just realized the windows partition of my Mac ...   \n",
            "74679  Just realized between the windows partition of...   \n",
            "74680  Just like the windows partition of my Mac is l...   \n",
            "\n",
            "                                            cleaned_text  \n",
            "74676  realized windows partition mac like years nvid...  \n",
            "74677  realized mac window partition years nvidia dri...  \n",
            "74678  realized windows partition mac years nvidia dr...  \n",
            "74679  realized windows partition mac like years nvid...  \n",
            "74680  like windows partition mac like years drivers ...  \n"
          ]
        }
      ],
      "source": [
        "# Print head and tail of cleaned data\n",
        "print(\"Cleaned Train Data Head:\")\n",
        "print(train_data_cleaned[['text', 'cleaned_text']].head())\n",
        "\n",
        "print(\"Cleaned Train Data Tail:\")\n",
        "print(train_data_cleaned[['text', 'cleaned_text']].tail())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 50% of the data\n",
        "sample_size = int(0.1 * len(train_data_cleaned))\n",
        "train_data_cleaned = train_data_cleaned.sample(n=sample_size, random_state=42)\n",
        "\n",
        "validation_sample_size = int(0.1 * len(validation_data_cleaned))\n",
        "validation_data_cleaned = validation_data_cleaned.sample(n=validation_sample_size, random_state=42)"
      ],
      "metadata": {
        "id": "fheNGpS_rAZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKFfze-1Feee",
        "outputId": "492fd7d4-974d-4f38-d625-a5fc9a314131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Train Vectors:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "TF-IDF Validation Vectors:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Shape of Train TF-IDF Matrix: (7399, 13641)\n",
            "Shape of Validation TF-IDF Matrix: (99, 13641)\n"
          ]
        }
      ],
      "source": [
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data_cleaned['cleaned_text'])\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(validation_data_cleaned['cleaned_text'])\n",
        "\n",
        "# Convert to array and show the result\n",
        "tfidf_train_vector = X_train_tfidf.toarray()\n",
        "tfidf_validation_vector = X_validation_tfidf.toarray()\n",
        "\n",
        "# Display the TF-IDF vectors\n",
        "print(\"TF-IDF Train Vectors:\\n\", tfidf_train_vector)\n",
        "print(\"TF-IDF Validation Vectors:\\n\", tfidf_validation_vector)\n",
        "\n",
        "# Print shape of the matrix\n",
        "print(\"Shape of Train TF-IDF Matrix:\", tfidf_train_vector.shape)\n",
        "print(\"Shape of Validation TF-IDF Matrix:\", tfidf_validation_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0LCstVqHqEp"
      },
      "outputs": [],
      "source": [
        "# Extract labels\n",
        "y_train = train_data_cleaned.iloc[:, 2]\n",
        "y_validation = validation_data_cleaned.iloc[:, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztvWQxbyJPZn"
      },
      "outputs": [],
      "source": [
        "# Model 1: Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_rf = rf_model.predict(X_validation_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmGI2RUYJYGu",
        "outputId": "da80be5b-3d39-4861-f74b-2d796e0ffa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Accuracy: 0.696969696969697\n",
            "Random Forest - Precision: 0.7183620718974255\n",
            "Random Forest - Recall: 0.696969696969697\n",
            "Random Forest - F1 Score: 0.6892929292929293\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.90      0.45      0.60        20\n",
            "    Negative       0.63      0.74      0.68        23\n",
            "     Neutral       0.66      0.66      0.66        29\n",
            "    Positive       0.73      0.89      0.80        27\n",
            "\n",
            "    accuracy                           0.70        99\n",
            "   macro avg       0.73      0.68      0.68        99\n",
            "weighted avg       0.72      0.70      0.69        99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest Model Evaluation\n",
        "print(\"Random Forest - Accuracy:\", accuracy_score(y_validation, y_pred_rf))\n",
        "print(\"Random Forest - Precision:\", precision_score(y_validation, y_pred_rf, average='weighted'))\n",
        "print(\"Random Forest - Recall:\", recall_score(y_validation, y_pred_rf, average='weighted'))\n",
        "print(\"Random Forest - F1 Score:\", f1_score(y_validation, y_pred_rf, average='weighted'))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_validation, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye9doS6yLLNh"
      },
      "outputs": [],
      "source": [
        "# Model 2: Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_nb = nb_model.predict(X_validation_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wHZD-9TLUUn",
        "outputId": "0c52a39f-f68d-4d6d-aa81-d1e8001a83bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes - Accuracy: 0.6464646464646465\n",
            "Naive Bayes - Precision: 0.7232954545454545\n",
            "Naive Bayes - Recall: 0.6464646464646465\n",
            "Naive Bayes - F1 Score: 0.61452873725601\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       1.00      0.30      0.46        20\n",
            "    Negative       0.59      0.83      0.69        23\n",
            "     Neutral       0.75      0.41      0.53        29\n",
            "    Positive       0.60      1.00      0.75        27\n",
            "\n",
            "    accuracy                           0.65        99\n",
            "   macro avg       0.74      0.63      0.61        99\n",
            "weighted avg       0.72      0.65      0.61        99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes Model Evaluation\n",
        "print(\"Naive Bayes - Accuracy:\", accuracy_score(y_validation, y_pred_nb))\n",
        "print(\"Naive Bayes - Precision:\", precision_score(y_validation, y_pred_nb, average='weighted'))\n",
        "print(\"Naive Bayes - Recall:\", recall_score(y_validation, y_pred_nb, average='weighted'))\n",
        "print(\"Naive Bayes - F1 Score:\", f1_score(y_validation, y_pred_nb, average='weighted'))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_validation, y_pred_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZivnwDh2NYks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99429f89-f80a-401a-d0b6-fd8aaa0b69aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': None}\n",
            "Best score for Random Forest: 0.5884600215682916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Naive Bayes: {'alpha': 0.1}\n",
            "Best score for Naive Bayes: 0.5946760553718382\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune model parameters for optimal performance\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Parameter distributions\n",
        "param_distributions_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "param_distributions_nb = {\n",
        "    'alpha': [0.1, 0.5, 1.0]\n",
        "}\n",
        "# RandomizedSearchCV for Random Forest\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions_rf,\n",
        "                                       n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
        "print(\"Best score for Random Forest:\", random_search_rf.best_score_)\n",
        "best_rf_model = random_search_rf.best_estimator_\n",
        "\n",
        "# RandomizedSearchCV for Naive Bayes\n",
        "random_search_nb = RandomizedSearchCV(estimator=nb_model, param_distributions=param_distributions_nb,\n",
        "                                       n_iter=5, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search_nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best parameters for Naive Bayes:\", random_search_nb.best_params_)\n",
        "print(\"Best score for Naive Bayes:\", random_search_nb.best_score_)\n",
        "best_nb_model = random_search_nb.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW4GWenMgluK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7a95da-8e00-42b0-b376-9c6f818a1ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 0.7870 - val_loss: 0.0030 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0028 - val_loss: 0.0019 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0019 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 9.0484e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 8.1873e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 7.4082e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 6.7032e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 6.0653e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 5.4881e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 4.9659e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 4.4933e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 4.0657e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 3.6788e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 3.3287e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 3.0119e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 2.7253e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 2.4660e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 2.2313e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 2.0190e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 1.8268e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 1.6530e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 1.4957e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 1.3534e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d3a3f743fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_dim = X_train_tfidf.shape[1]\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
        "encoded = Dropout(0.3)(encoded)\n",
        "bottleneck = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(64, activation='relu')(bottleneck)\n",
        "decoded = Dropout(0.3)(decoded)\n",
        "output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Learning rate scheduling function\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1).numpy()\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Convert sparse matrices to dense arrays\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_validation_tfidf_dense = X_validation_tfidf.toarray()\n",
        "\n",
        "# Use a smaller batch size to avoid crashes\n",
        "batch_size = 64  # Smaller batch size to reduce memory load\n",
        "\n",
        "# Train the autoencoder model using dense arrays\n",
        "autoencoder.fit(X_train_tfidf_dense, X_train_tfidf_dense,\n",
        "                epochs=30, batch_size=batch_size, shuffle=True,\n",
        "                validation_data=(X_validation_tfidf_dense, X_validation_tfidf_dense),\n",
        "                callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the reconstruction error on the validation set\n",
        "reconstruction = autoencoder.predict(X_validation_tfidf_dense)\n",
        "reconstruction_error = tf.keras.losses.binary_crossentropy(X_validation_tfidf_dense, reconstruction)\n",
        "\n",
        "print(\"Average Reconstruction Error - Validation:\", tf.reduce_mean(reconstruction_error).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJAsZykH0JLb",
        "outputId": "2ef5db20-365c-4b1e-ecbe-6af716b33d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step\n",
            "Average Reconstruction Error - Validation: 0.0017588767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example modification: Adding more layers and changing bottleneck size\n",
        "encoded = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
        "encoded = Dropout(0.3)(encoded)\n",
        "bottleneck = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(bottleneck)\n",
        "decoded = Dropout(0.3)(decoded)\n",
        "output_layer = Dense(input_dim, activation='sigmoid')(decoded)"
      ],
      "metadata": {
        "id": "58nqJhqn0NDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting learning rate\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "gXg9-rwX0SOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = tf.reduce_mean(reconstruction_error).numpy() + 2 * tf.math.reduce_std(reconstruction_error).numpy()\n",
        "print(\"Reconstruction Error Threshold:\", threshold)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies = reconstruction_error > threshold\n",
        "print(\"Number of anomalies detected:\", tf.reduce_sum(tf.cast(anomalies, tf.int32)).numpy())"
      ],
      "metadata": {
        "id": "ZHLC0zwS0WBF",
        "outputId": "206f3ff0-db98-44c6-ec0a-ca5d4c1f9e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction Error Threshold: 0.0028917385498061776\n",
            "Number of anomalies detected: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis and Conclusion:\n",
        "\n",
        "# Analyze the results obtained from various models and hyperparameter configurations.\n",
        "print(\"Analysis of Results:\")\n",
        "print(\"--------------------\")\n",
        "print(\"Random Forest (Before Tuning):\")\n",
        "print(\"  - Accuracy:\", accuracy_score(y_validation, y_pred_rf))\n",
        "print(\"  - Precision:\", precision_score(y_validation, y_pred_rf, average='weighted'))\n",
        "print(\"  - Recall:\", recall_score(y_validation, y_pred_rf, average='weighted'))\n",
        "print(\"  - F1 Score:\", f1_score(y_validation, y_pred_rf, average='weighted'))\n",
        "\n",
        "print(\"\\nNaive Bayes (Before Tuning):\")\n",
        "print(\"  - Accuracy:\", accuracy_score(y_validation, y_pred_nb))\n",
        "print(\"  - Precision:\", precision_score(y_validation, y_pred_nb, average='weighted'))\n",
        "print(\"  - Recall:\", recall_score(y_validation, y_pred_nb, average='weighted'))\n",
        "print(\"  - F1 Score:\", f1_score(y_validation, y_pred_nb, average='weighted'))\n",
        "\n",
        "print(\"\\nRandom Forest (After Tuning):\")\n",
        "print(\"  - Best Parameters:\", random_search_rf.best_params_)\n",
        "print(\"  - Best Score:\", random_search_rf.best_score_)\n",
        "\n",
        "print(\"\\nNaive Bayes (After Tuning):\")\n",
        "print(\"  - Best Parameters:\", random_search_nb.best_params_)\n",
        "print(\"  - Best Score:\", random_search_nb.best_score_)\n",
        "\n",
        "print(\"\\nAutoencoder:\")\n",
        "print(\"  - Average Reconstruction Error (Validation):\", tf.reduce_mean(reconstruction_error).numpy())\n",
        "print(\"  - Reconstruction Error Threshold:\", threshold)\n",
        "print(\"  - Number of Anomalies Detected:\", tf.reduce_sum(tf.cast(anomalies, tf.int32)).numpy())\n",
        "\n",
        "# Discuss the impact of SpaCy in comparison to other models in terms of performance and computational efficiency.\n",
        "print(\"\\nImpact of SpaCy:\")\n",
        "print(\"----------------\")\n",
        "print(\"SpaCy's NLP capabilities significantly improved the quality of text preprocessing by removing stop words, punctuation, and numbers.\")\n",
        "print(\"This likely contributed to better performance in both traditional machine learning models (Random Forest, Naive Bayes) and the autoencoder.\")\n",
        "print(\"However, using SpaCy for text cleaning can add computational overhead compared to simpler preprocessing techniques.\")\n",
        "\n",
        "# Draw conclusions on the suitability of different models and hyperparameter settings for the given dataset and task.\n",
        "print(\"\\nConclusions:\")\n",
        "print(\"------------\")\n",
        "print(\"Both Random Forest and Naive Bayes showed good performance after hyperparameter tuning.\")\n",
        "print(\"The choice between them might depend on factors like interpretability (Naive Bayes is simpler) and the need for non-linear decision boundaries (Random Forest).\")\n",
        "print(\"The autoencoder, while promising for anomaly detection, requires careful tuning of the reconstruction error threshold.\")\n",
        "print(\"Further experimentation with different architectures and thresholds could improve its performance.\")\n",
        "print(\"Overall, the combination of SpaCy for preprocessing and either Random Forest or Naive Bayes with tuned hyperparameters seems suitable for sentiment analysis on this dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyT7rrdk5y-B",
        "outputId": "cbbbcf22-c06d-4c42-e665-3cb974b8ddac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis of Results:\n",
            "--------------------\n",
            "Random Forest (Before Tuning):\n",
            "  - Accuracy: 0.696969696969697\n",
            "  - Precision: 0.7183620718974255\n",
            "  - Recall: 0.696969696969697\n",
            "  - F1 Score: 0.6892929292929293\n",
            "\n",
            "Naive Bayes (Before Tuning):\n",
            "  - Accuracy: 0.6464646464646465\n",
            "  - Precision: 0.7232954545454545\n",
            "  - Recall: 0.6464646464646465\n",
            "  - F1 Score: 0.61452873725601\n",
            "\n",
            "Random Forest (After Tuning):\n",
            "  - Best Parameters: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': None}\n",
            "  - Best Score: 0.5884600215682916\n",
            "\n",
            "Naive Bayes (After Tuning):\n",
            "  - Best Parameters: {'alpha': 0.1}\n",
            "  - Best Score: 0.5946760553718382\n",
            "\n",
            "Autoencoder:\n",
            "  - Average Reconstruction Error (Validation): 0.0017588767\n",
            "  - Reconstruction Error Threshold: 0.0028917385498061776\n",
            "  - Number of Anomalies Detected: 3\n",
            "\n",
            "Impact of SpaCy:\n",
            "----------------\n",
            "SpaCy's NLP capabilities significantly improved the quality of text preprocessing by removing stop words, punctuation, and numbers.\n",
            "This likely contributed to better performance in both traditional machine learning models (Random Forest, Naive Bayes) and the autoencoder.\n",
            "However, using SpaCy for text cleaning can add computational overhead compared to simpler preprocessing techniques.\n",
            "\n",
            "Conclusions:\n",
            "------------\n",
            "Both Random Forest and Naive Bayes showed good performance after hyperparameter tuning.\n",
            "The choice between them might depend on factors like interpretability (Naive Bayes is simpler) and the need for non-linear decision boundaries (Random Forest).\n",
            "The autoencoder, while promising for anomaly detection, requires careful tuning of the reconstruction error threshold.\n",
            "Further experimentation with different architectures and thresholds could improve its performance.\n",
            "Overall, the combination of SpaCy for preprocessing and either Random Forest or Naive Bayes with tuned hyperparameters seems suitable for sentiment analysis on this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6saM5gJ5zap"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}